{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a conda environment (or venv if you prefer that method) to work in, many ways to run this command:\n",
    "`conda create --name <name_of_env> python==3.11`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When running next command, will be prompted to install ipykernel package, do that first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (0.3.3)\n",
      "Requirement already satisfied: langchain-ollama in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: langchain-community in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (0.3.2)\n",
      "Requirement already satisfied: langchain-huggingface in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (0.1.0)\n",
      "Requirement already satisfied: faiss-cpu in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (1.9.0)\n",
      "Requirement already satisfied: huggingface_hub in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (0.25.2)\n",
      "Requirement already satisfied: pandas in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dotenv in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain) (3.10.10)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain) (0.3.12)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain) (0.1.136)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: ollama<1,>=0.3.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain-ollama) (0.3.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain-community) (2.6.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain-huggingface) (3.2.0)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain-huggingface) (0.20.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain-huggingface) (4.45.2)\n",
      "Requirement already satisfied: packaging in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: filelock in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.15.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.5.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.5.2)\n",
      "Requirement already satisfied: scipy in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.14.1)\n",
      "Requirement already satisfied: Pillow in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (11.0.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from transformers>=4.39.0->langchain-huggingface) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.5)\n",
      "Requirement already satisfied: anyio in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain) (3.0.0)\n",
      "Requirement already satisfied: networkx in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the core packages for LangChain, Ollama, and related components\n",
    "%pip install -U langchain langchain-ollama langchain-community langchain-huggingface faiss-cpu huggingface_hub pandas python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import time\n",
    "import atexit\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define Ollama generation arguments\n",
    "generation_kwargs = {\n",
    "    \"max_tokens\": 500,\n",
    "    \"temperature\": 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization of Vector Stores and Embeddings (Create vector storage that will be drawn from for RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (4.66.5)\n",
      "Requirement already satisfied: tenacity in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (8.5.0)\n",
      "Requirement already satisfied: tiktoken in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bradyneeley/miniforge3/envs/rag_demo/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install remaining packages\n",
    "%pip install tqdm tenacity tiktoken\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import traceback\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential, retry_if_exception_type\n",
    "from huggingface_hub import login\n",
    "import tiktoken\n",
    "\n",
    "# Add a huggingface token to .env file \n",
    "load_dotenv()\n",
    "\n",
    "# Use the Hugging Face access token for authentication\n",
    "hf_access_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "#PATH_TO_DATA = \"hf://datasets/zhengyun21/PMC-Patients/PMC-Patients.csv\"\n",
    "PATH_TO_DATA = \"hf://datasets/ncbi/Open-Patients/Open-Patients.jsonl\"\n",
    "BATCH_SIZE = 10  # Adjust based on your needs\n",
    "PATH_TO_VECTORDB = \"./db/faiss_index\"\n",
    "TOKEN_LIMIT = 125  # Maximum number of tokens per chunk\n",
    "\n",
    "# Initialize tokenizer (assuming LLaMA tokenizer, you can replace with a specific tokenizer you use)\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # Replace with the correct tokenizer for your model if different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Creating documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating documents: 100%|██████████| 10/10 [00:00<00:00, 3282.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vector database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batches: 100%|██████████| 2/2 [00:01<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully embedded 12 out of 12 documents\n",
      "Saving vector database locally...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, token_limit):\n",
    "    \"\"\"\n",
    "    Splits a long text into chunks of no more than `token_limit` tokens.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to be chunked.\n",
    "        token_limit (int): Maximum number of tokens per chunk.\n",
    "\n",
    "    Returns:\n",
    "        List of chunks (str) each within the token limit.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(tokens), token_limit):\n",
    "        chunk_tokens = tokens[i:i + token_limit]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def create_documents(row):\n",
    "    \"\"\"\n",
    "    Create documents, chunked by the token limit. One row can result in multiple documents.\n",
    "    \n",
    "    Args:\n",
    "        row: A row of the dataframe (with _id and description fields).\n",
    "    \n",
    "    Returns:\n",
    "        List of Documents, one for each chunk.\n",
    "    \"\"\"\n",
    "    chunks = chunk_text(row['description'], TOKEN_LIMIT)  # Access the 'description' field\n",
    "    \n",
    "    return [\n",
    "        Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\"_id\": row['_id']}  # Use _id as metadata\n",
    "        )\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "@retry(\n",
    "    wait=wait_random_exponential(min=1, max=60),\n",
    "    stop=stop_after_attempt(6),\n",
    "    retry=retry_if_exception_type(Exception)\n",
    ")\n",
    "def embed_with_backoff(embeddings, texts):\n",
    "    return embeddings.embed_documents(texts)\n",
    "\n",
    "def process_batch(batch, embeddings):\n",
    "    try:\n",
    "        return embed_with_backoff(embeddings, batch)\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding batch: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Build vector database\n",
    "def main():\n",
    "    if os.path.exists(PATH_TO_VECTORDB):\n",
    "        print('Vector database already exists')\n",
    "    else:\n",
    "        try:\n",
    "            print('Loading data')\n",
    "            # Loading JSON data instead of CSV\n",
    "            df = pd.read_json(PATH_TO_DATA, lines=True)  # Assuming the JSON is in \"lines\" format\n",
    "\n",
    "            # Limiting to the first 1000 rows\n",
    "            df = df.head(10)\n",
    "\n",
    "            total_rows = len(df)\n",
    "\n",
    "            print('Creating documents')\n",
    "            # Generate documents, including chunking where necessary\n",
    "            documents = []\n",
    "            for _, row in tqdm(df.iterrows(), total=total_rows, desc=\"Creating documents\"):\n",
    "                documents.extend(create_documents(row))\n",
    "\n",
    "            print('Building vector database...')\n",
    "            # Initialize Hugging Face embeddings with the same model used in evaluate_matches.py\n",
    "            embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=\"mixedbread-ai/mxbai-embed-large-v1\",\n",
    "                model_kwargs={\"trust_remote_code\": True}\n",
    "            )\n",
    "\n",
    "            # Batch processing for embeddings\n",
    "            batches = [documents[i:i + BATCH_SIZE] for i in range(0, len(documents), BATCH_SIZE)]\n",
    "\n",
    "            all_embeddings = []\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                futures = [executor.submit(process_batch, [doc.page_content for doc in batch], embeddings) for batch in batches]\n",
    "                for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing batches\"):\n",
    "                    result = future.result()\n",
    "                    if result is not None:\n",
    "                        all_embeddings.extend(result)\n",
    "\n",
    "            # Filter out documents for which we couldn't get embeddings\n",
    "            valid_docs = [doc for doc, emb in zip(documents, all_embeddings) if emb is not None]\n",
    "            valid_embeddings = [emb for emb in all_embeddings if emb is not None]\n",
    "\n",
    "            print(f\"Successfully embedded {len(valid_docs)} out of {len(documents)} documents\")\n",
    "\n",
    "            # Create the FAISS vector store with Hugging Face embeddings\n",
    "            vector_store = FAISS.from_embeddings(\n",
    "                text_embeddings=list(zip([doc.page_content for doc in valid_docs], valid_embeddings)),\n",
    "                embedding=embeddings,\n",
    "                metadatas=[doc.metadata for doc in valid_docs]\n",
    "            )\n",
    "\n",
    "            retriever = vector_store.as_retriever()\n",
    "\n",
    "            print('Saving vector database locally...')\n",
    "            vector_store.save_local(PATH_TO_VECTORDB)\n",
    "\n",
    "            print('Done')\n",
    "            return vector_store, retriever\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            print(\"Traceback:\")\n",
    "            print(traceback.format_exc())\n",
    "            return \"Error occurred\", \"Error occurred\", [], []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or load the preindexed by Brady PMC-Patients dataset (This is your vector storage or indexed documents/text, what RAG draws from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 00:54:06,379 - INFO - Use pytorch device_name: mps\n",
      "2024-10-18 00:54:06,380 - INFO - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings and FAISS vector store\n",
    "PATH_TO_VECTORDB = \"./db/faiss_index\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"mixedbread-ai/mxbai-embed-large-v1\",\n",
    "    model_kwargs={\"trust_remote_code\": True}\n",
    ")\n",
    "\n",
    "vector_store = FAISS.load_local(PATH_TO_VECTORDB, embeddings, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Ollama (Invoking LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ollama(prompt, generation_kwargs, output_format=\"text\"):\n",
    "    \"\"\"\n",
    "    Sends a prompt to the Ollama model using langchain-ollama integration.\n",
    "    Args:\n",
    "    prompt (str): The generated prompt to send to the model.\n",
    "    generation_kwargs (dict): Generation settings.\n",
    "    output_format (str): The expected format of the output, either \"text\" or \"json\".\n",
    "    \"\"\"\n",
    "    model = OllamaLLM(model=\"llama3.2:3b\")\n",
    "    chat_prompt = ChatPromptTemplate.from_template(prompt)\n",
    "    chain = chat_prompt | model\n",
    "\n",
    "    response = chain.invoke({\"question\": prompt})\n",
    "\n",
    "    if output_format == \"json\":\n",
    "        try:\n",
    "            result = json.loads(response)\n",
    "            return result\n",
    "        except json.JSONDecodeError:\n",
    "            logger.error(\"Failed to decode JSON response from model.\")\n",
    "            return {\n",
    "                \"error\": \"Error in generating response or unexpected response format.\"\n",
    "            }\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Generation (Instructions for the LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(article_text, query_ollama, generation_kwargs):\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following patient case in a single paragraph, focusing on relevant phenotypes, diagnoses, and other attributes of the patient.\n",
    "    The summary should be concise and retain important scientific or clinical terminology.\n",
    "    ### Article Text:\n",
    "    {article_text}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    response = query_ollama(prompt, generation_kwargs, output_format=\"text\")\n",
    "    return response\n",
    "\n",
    "\n",
    "def generate_potential_causes(patient_case, article_summaries):\n",
    "    examples = \"\\n\\n\".join([f\"### Example Case {i+1}:\\n{summary}\" for i, summary in enumerate(article_summaries)])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    A user has provided a detailed patient's case with specific symptoms and medical history.\n",
    "    Below are some examples of similar patient cases from medical literature.\n",
    "    \n",
    "    Your task is to identify potential causes for the symptoms described in the user's case, using the examples as references.\n",
    "\n",
    "    ### User-Provided Patient Case:\n",
    "    {patient_case}\n",
    "    \n",
    "    {examples}\n",
    "\n",
    "    Please provide:\n",
    "    - A list of potential causes based on the information.\n",
    "    - Brief explanations for each cause.\n",
    "    \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 00:55:27,957 - INFO - Patient Case: 64-year-old obese female with diagnosis of diabetes mellitus and persistently elevated HbA1c\n",
      "2024-10-18 00:55:29,065 - INFO - Related patient cases saved to related_patient_cases.txt in human-readable format.\n",
      "2024-10-18 00:55:31,443 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2024-10-18 00:55:40,726 - INFO - Potential causes saved to potential_causes.txt.\n",
      "2024-10-18 00:55:40,726 - INFO - Potential causes identified and saved.\n"
     ]
    }
   ],
   "source": [
    "def ask_me_potential_causes(patient_narrative, max_retries=10, delay=2, k=4):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            logger.info(f\"Patient Case: {patient_narrative}\")\n",
    "            # Retrieve similar cases from FAISS vector store\n",
    "            docs = vector_store.similarity_search_with_score(patient_narrative, k=k)\n",
    "            article_summaries = [doc.page_content for doc, score in docs]\n",
    "\n",
    "            # Save related cases in human-readable format to a .txt file\n",
    "            output_path_cases_txt = \"related_patient_cases.txt\"\n",
    "            with open(output_path_cases_txt, 'w') as txt_file:\n",
    "                txt_file.write(\"Related Patient Cases (Double-Spaced):\\n\\n\")\n",
    "                for i, summary in enumerate(article_summaries):\n",
    "                    txt_file.write(f\"Case {i+1}:\\n{summary}\\n\\n\")\n",
    "            logger.info(f\"Related patient cases saved to {output_path_cases_txt} in human-readable format.\")\n",
    "\n",
    "            # Generate a prompt to find potential causes\n",
    "            prompt = generate_potential_causes(patient_narrative, article_summaries)\n",
    "            potential_causes = query_ollama(prompt, generation_kwargs, output_format=\"text\")\n",
    "\n",
    "            # Save potential causes to a .txt file\n",
    "            output_path_causes_txt = \"potential_causes.txt\"\n",
    "            with open(output_path_causes_txt, 'w') as txt_file:\n",
    "                txt_file.write(\"Potential Causes for Patient Case:\\n\\n\")\n",
    "                txt_file.write(potential_causes)\n",
    "            logger.info(f\"Potential causes saved to {output_path_causes_txt}.\")\n",
    "\n",
    "            return potential_causes\n",
    "\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            logger.error(f\"Error occurred: {e}. Retrying {retries}/{max_retries}...\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "    logger.warning(\"Max retries reached, could not complete the request.\")\n",
    "    return \"Max retries reached, could not complete the request.\"\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    patient_narrative = \"64-year-old obese female with diagnosis of diabetes mellitus and persistently elevated HbA1c\"\n",
    "\n",
    "    result = ask_me_potential_causes(patient_narrative)\n",
    "    logger.info(\"Potential causes identified and saved.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
